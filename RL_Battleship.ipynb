{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RL-Battleship",
      "provenance": [],
      "collapsed_sections": [
        "p3F2i3OPmGoq",
        "duKgmzJImVJ_",
        "WssJXj55dex2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL_7O-oQUcaQ"
      },
      "source": [
        "# !pip install gym\n",
        "# !pip install tensorflow\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from gym import spaces\n",
        "from google.colab import files\n",
        "import torch\n",
        "import random"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3F2i3OPmGoq"
      },
      "source": [
        "# Battleship Environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oSYkWwfVB7S"
      },
      "source": [
        "class BatteshipEnv(gym.Env):\n",
        "  def __init__(self, grid_size, ships_board):\n",
        "    \"\"\"\n",
        "      Initialize a Battleship environment\n",
        "      :param grid_size: With grid_size = n we have a board of n*n\n",
        "      :param ships_board: Given board with placed ships\n",
        "      :return BatteshipEnv object\n",
        "    \"\"\"\n",
        "    super(BatteshipEnv, self).__init__()\n",
        "    self.grid_size = grid_size\n",
        "    self.ships_board = ships_board\n",
        "    self.board = np.zeros((self.grid_size,self.grid_size), dtype='int') # Currently playing board\n",
        "    self.s = (0,0,0)  # Currently state: a tuple representing (x_coordinate, y_coordinate, health)\n",
        "    self.max_health = int(np.max(ships_board)) # The maximum health h among the ships\n",
        "    self.observation_space = spaces.Discrete(self.grid_size * self.grid_size) # Obeservation space\n",
        "    self.action_space = spaces.Tuple((spaces.Discrete(self.grid_size),spaces.Discrete(self.grid_size))) # Action space\n",
        "    self.nS = self.grid_size * self.grid_size * (self.max_health+3) # Size of state space n*n*(hmax+3)\n",
        "    self.nA = self.grid_size * self.grid_size # Size of action space n*n\n",
        "    self.number_of_hit_to_win = np.sum(self.ships_board) # Number of hit to win the game = sum of health of all ships   \n",
        "    self.number_of_hit = 0  # Current number of hit \n",
        "    self.destroyed = int(self.max_health + 1) # Mark a square as destroyed ship\n",
        "    self.empty =  int(self.max_health + 1) # Mark a square as square\n",
        "    \n",
        "  def calculate_reward(self, action):\n",
        "    \"\"\" \n",
        "      Return reward for an action\n",
        "      :param action: action taken\n",
        "      :return reward \n",
        "    \"\"\" \n",
        "    x = action[0] # x coordinate of the action\n",
        "    y = action[1] # y coordinate of the action\n",
        "    if (self.board[x,y] == self.empty) | (self.board[x,y] == self.destroyed) :\n",
        "      return -1 # If the agent hits an empty square or a square where a ship has sunk\n",
        "    else:\n",
        "      return 0  # Otherwise\n",
        "\n",
        "  def update_board(self, action):\n",
        "    \"\"\"\n",
        "      Update the playing board\n",
        "      :param action: action taken\n",
        "      :return the current state after taking the action\n",
        "    \"\"\"\n",
        "    x = action[0] # x coordinate of the action\n",
        "    y = action[1] # y coordinate of the action\n",
        "    # If the agent hits an empty square\n",
        "    if self.board[x,y] == self.empty:\n",
        "      self.s = (action[0], action[1], int(self.empty))\n",
        "    # If the agent hits a square where a ship has sunk \n",
        "    elif self.board[x,y] == self.destroyed:\n",
        "      self.s = (action[0], action[1], int(self.destroyed))\n",
        "    else:\n",
        "      # If the agent hits a square with a ship \n",
        "      if self.ships_board[x,y] != 0:\n",
        "        # Make 1 more hit\n",
        "        if self.board[x,y] < self.ships_board[x,y]:\n",
        "          self.board[x,y] += 1 # One more hit\n",
        "          self.number_of_hit += 1\n",
        "        # The ship has sunk already\n",
        "        else:\n",
        "          self.board[x,y] = self.destroyed\n",
        "      # The agent hits an empty square\n",
        "      else:\n",
        "        self.board[x,y] = self.empty\n",
        "      # Update current state\n",
        "      self.s = (action[0], action[1], int(self.board[x,y]))\n",
        "    return self.s\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\" \n",
        "      Take a step by executing an action\n",
        "      :param actionL action taken\n",
        "      :return next_state: the state after taking the action\n",
        "      :return reward: reward for taking action from the current state\n",
        "      :return done: Whether all the ships have been destroyed \n",
        "      :return info: none\n",
        "    \"\"\"\n",
        "    next_state = self.update_board(action)\n",
        "    reward = self.calculate_reward(action)\n",
        "    done = bool(self.number_of_hit == self.number_of_hit_to_win)\n",
        "    info = {}\n",
        "    return next_state, reward, done, info\n",
        "\n",
        "  def observe(self):\n",
        "    \"\"\"\n",
        "      Return the current state\n",
        "    \"\"\"\n",
        "    return self.s\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "      Reset the environment\n",
        "      :return the initial playing board\n",
        "    \"\"\"\n",
        "    self.board = np.zeros((self.grid_size,self.grid_size))\n",
        "    self.number_of_hit = 0\n",
        "    self.s = (np.random.randint(0, self.grid_size),np.random.randint(0, self.grid_size),0)\n",
        "    return self.board\n",
        "\n",
        "  def render(self, mode='human'):\n",
        "    \"\"\"\n",
        "      Render the current playing board\n",
        "    \"\"\"\n",
        "    for i in range(self.grid_size):\n",
        "      print(\"-------------------------------------------\")\n",
        "      line = \"\"\n",
        "      for j in range(self.grid_size):     \n",
        "        line += \" | \"\n",
        "        if self.board[i,j] == -1:\n",
        "          line += \"O\"\n",
        "        elif self.board[i,j] == 0:\n",
        "          line += \" \"\n",
        "        else:\n",
        "          line += str(int(self.board[i,j]))\n",
        "      line += \" | \"\n",
        "      print(line)\n",
        "\n",
        "##### HELPER FUNCTIONS #####\n",
        "def plot_results(list_cumulative_rewards, list_cumulative_steps, algo_name, download_plots):\n",
        "  \"\"\"\n",
        "    Plotting the results\n",
        "    :param list_cumulative_rewards: list of cumulative rewards of all episodes\n",
        "    :param list_cumulative_steps: list of number of actions of all episodes\n",
        "    :param algo_name: the name of the learning algorithm\n",
        "    :param download_plots: \"True\" if this notebook is running on Google Colab and you want to save the figures to your computer, \"False\" otherwise\n",
        "  \"\"\"\n",
        "  plt.plot(list_cumulative_rewards, label=\"legend\")\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Cumulative rewards')\n",
        "  plt.axhline(24, label ='Optimal Number of Actions', color ='red') \n",
        "  if download_plots:\n",
        "    plt.savefig(algo_name+\"-cum-rewards.eps\", format='eps', dpi=1200)\n",
        "    files.download(algo_name+\"-cum-rewards.eps\")\n",
        "    plt.clf()\n",
        "  else:\n",
        "    plt.show()\n",
        "  plt.plot(list_cumulative_steps, label=\"legend\")\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Number of actions to finish the game')\n",
        "  plt.axhline(24, label ='Optimal Number of Actions', color ='red') \n",
        "  if download_plots:\n",
        "    plt.savefig(algo_name+\"-actions.eps\", format='eps', dpi=1200)\n",
        "    files.download(algo_name+\"-actions.eps\")\n",
        "    plt.clf() \n",
        "  else:\n",
        "    plt.show()\n",
        "\n",
        "def env_generator():\n",
        "  \"\"\"\n",
        "    Generate a testing environment with a board of 10x10 squares and 10 ships\n",
        "    1 ship which needs 1 hit to be sunk\n",
        "    4 ships which need 2 hits to be sunk\n",
        "    5 ships which need 3 hits to be sunk\n",
        "  \"\"\"\n",
        "  grid_size = 10\n",
        "  ships_board = np.zeros((grid_size, grid_size), dtype='int')\n",
        "  ships_board[0,9] = 1\n",
        "  ships_board[1,3] = 2 \n",
        "  ships_board[7,2] = 2 \n",
        "  ships_board[3,1] = 2\n",
        "  ships_board[6,7] = 2\n",
        "  ships_board[3,5] = 3 \n",
        "  ships_board[9,9] = 3 \n",
        "  ships_board[4,5] = 3 \n",
        "  ships_board[9,4] = 3\n",
        "  ships_board[5,8] = 3\n",
        "  env = BatteshipEnv(grid_size, ships_board)\n",
        "  return env\n"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duKgmzJImVJ_"
      },
      "source": [
        "# Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAcxGcQrnkd8"
      },
      "source": [
        "class QLearning():\n",
        "  def __init__(self, env):\n",
        "    self.env = env\n",
        "    self.learning_rate = 0.05\n",
        "    self.epsilon = 1.0 # For deciding exploitation or exploration\n",
        "    self.epsilon_decay_rate = 0.99 # Epsilon is decayed after each episode with a fixed rate\n",
        "    self.discount = 0.5 # The weight for future rewards\n",
        "    self.Q_values = self.Q_values = np.zeros([self.env.nS, self.env.nA]) # Q table\n",
        "\n",
        "  def update(self, state, action, reward):\n",
        "    \"\"\" Update Q-table - ON POLICY\n",
        "        :param state: previous state before taking the action\n",
        "        :param action: action taken\n",
        "        :param reward: reward for taking the action from the current state\n",
        "    \"\"\"\n",
        "    # Get current state after taking the action and convert from 3D index to 1D index\n",
        "    new_state = self.env.observe()\n",
        "    idx_new_state = np.ravel_multi_index([new_state[0], new_state[1], new_state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    \n",
        "    # Convert to 1D index\n",
        "    idx_state = np.ravel_multi_index([state[0], state[1], state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    idx_action = np.ravel_multi_index([action[0], action[1]],(env.grid_size, env.grid_size))\n",
        "\n",
        "    #Update Q values - OFF POLICY\n",
        "    self.Q_values[idx_state][idx_action] = self.Q_values[idx_state][idx_action] + self.learning_rate * (reward + self.discount * max(self.Q_values[idx_new_state]) - self.Q_values[idx_state][idx_action])\n",
        "\n",
        "  def get_action(self, state):\n",
        "    \"\"\" Return an action to take based on epsilon (greedy or random action)\n",
        "        :param state: the current state\n",
        "        :return action: next action to take\n",
        "    \"\"\"\n",
        "    # Convert to 1D index\n",
        "    idx_state = np.ravel_multi_index([state[0], state[1], state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    \n",
        "    random_number = np.random.uniform()\n",
        "    if random_number < self.epsilon:\n",
        "      # Random action\n",
        "      return (np.random.randint(0, self.env.grid_size), np.random.randint(0, self.env.grid_size)) \n",
        "    else:\n",
        "      # Greedy action\n",
        "      idx_action = np.argmax(self.Q_values[idx_state]) \n",
        "      return np.unravel_index(idx_action,(self.env.grid_size,self.env.grid_size)) # Return 2D index of the action\n",
        "\n",
        "##### HELPER FUNCTIONS #####\n",
        "def test_Q_Learning(env, number_of_episodes, download_plots):\n",
        "  \"\"\"\n",
        "    A complete test of Q-Learning agent\n",
        "    :param env: the environment where the agent plays\n",
        "    :param number_of_episodes: number of episodes\n",
        "    :param download_plots: \"True\" if this notebook is running on Google Colab and you want to save the figures to your computer, \"False\" otherwise\n",
        "  \"\"\"\n",
        "\n",
        "  agent = QLearning(env)  # Q-Learning Agent\n",
        "  list_cumulative_rewards = [] # list of cumulative rewards of all episodes\n",
        "  list_number_of_actions = [] # list of number of actions of all episodes\n",
        "\n",
        "  for ep in range (number_of_episodes):\n",
        "    sum_reward = 0\n",
        "    done = False\n",
        "    number_of_action = 0  # Current number of actions have been taken in this episode\n",
        "    while not done:\n",
        "      # Get current state\n",
        "      current_state = env.observe()\n",
        "\n",
        "      # Get next action\n",
        "      action = agent.get_action(current_state)\n",
        "\n",
        "      # Execute action and get feedback\n",
        "      observation, reward, done, info = env.step(action)\n",
        "\n",
        "      # Update Q-table\n",
        "      agent.update(current_state, action, reward)\n",
        "\n",
        "      # Visualize how the agent plays in the last episode\n",
        "      if ep==number_of_episodes-1:\n",
        "        i, j = np.unravel_index(action, (env.grid_size, env.grid_size))\n",
        "        print(\"Action: (\" + str(j) + \") Reward: \", reward)\n",
        "      \n",
        "      # Update reward and number of actions\n",
        "      sum_reward += reward\n",
        "      number_of_action += 1\n",
        "    list_cumulative_rewards.append(sum_reward)\n",
        "    list_number_of_actions.append(number_of_action)\n",
        "\n",
        "    # Print log\n",
        "    print(\"Episode \", ep+1, \" Cumulative reward: \", sum_reward, \" Number of actions: \", number_of_action)\n",
        "    \n",
        "    # Reset env after each episode\n",
        "    env.reset()\n",
        "\n",
        "    # Epsilon is decayed since the agent is getting more and more knowledge\n",
        "    agent.epsilon = agent.epsilon * agent.epsilon_decay_rate\n",
        "  \n",
        "  # Plot or download the results\n",
        "  plot_results(list_cumulative_rewards, list_number_of_actions, \"Q-Learning\", download_plots)\n"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WssJXj55dex2"
      },
      "source": [
        "# SARSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmca671mgLbx"
      },
      "source": [
        "class Sarsa():\n",
        "  def __init__(self, env):\n",
        "    self.env = env\n",
        "    self.learning_rate = 0.05\n",
        "    self.epsilon = 1.0 # For deciding exploitation or exploration\n",
        "    self.epsilon_decay_rate = 0.99 # Epsilon is decayed after each episode with a fixed rate\n",
        "    self.discount = 0.5 # The weight for future rewards\n",
        "\n",
        "    self.Q_values = np.zeros([self.env.nS, self.env.nA]) # Q table\n",
        "\n",
        "  def update(self, state, state2, reward, action, action2):\n",
        "    \"\"\" Update Q-table - ON POLICY\n",
        "        :param state: state before taking the action\n",
        "        :param action: next action to be taken\n",
        "        :param reward: reward for taking the action from the state\n",
        "        :param state2: the state that the agent enters after taking that action\n",
        "        :param action2: the next action the agent chooses in its new state2\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert all to 1-D indices\n",
        "    idx_state = np.ravel_multi_index([state[0], state[1], state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    idx_state2 = np.ravel_multi_index([state2[0], state2[1], state2[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    idx_action = np.ravel_multi_index([action[0], action[1]],(env.grid_size, env.grid_size))\n",
        "    idx_action2 = np.ravel_multi_index([action2[0], action2[1]],(env.grid_size, env.grid_size))\n",
        "\n",
        "    #Update Q values - ON POLICY\n",
        "    self.Q_values[idx_state][idx_action] = self.Q_values[idx_state][idx_action] \\\n",
        "                                          + self.learning_rate * (reward + self.discount * self.Q_values[idx_state2][idx_action2] - self.Q_values[idx_state][idx_action])\n",
        "\n",
        "  def get_action(self, state):\n",
        "    \"\"\" Return an action to take based on epsilon (greedy or random action)\n",
        "        :param state: the current state\n",
        "        :return action: next action to take\n",
        "    \"\"\"\n",
        "    # Convert to 1D index\n",
        "    idx_state = np.ravel_multi_index([state[0], state[1], state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    \n",
        "    random_number = np.random.uniform()\n",
        "    if random_number < self.epsilon:\n",
        "      # Random action\n",
        "      return (np.random.randint(0, self.env.grid_size), np.random.randint(0, self.env.grid_size))\n",
        "    else:\n",
        "      # Greedy action\n",
        "      idx_action = np.argmax(self.Q_values[idx_state])\n",
        "      return np.unravel_index(idx_action,(self.env.grid_size,self.env.grid_size)) # Return 2-D index of the action\n",
        "\n",
        "##### HELPER FUNCTIONS #####\n",
        "def test_SARSA(env, number_of_episodes, download_plots):\n",
        "  \"\"\"\n",
        "    A complete test of SARSA agent\n",
        "    :param env: the environment where the agent plays\n",
        "    :param number_of_episodes: number of episodes\n",
        "    :param download_plots: \"True\" if this notebook is running on Google Colab and you want to save the figures to your computer, \"False\" otherwise\n",
        "  \"\"\"\n",
        "  \n",
        "  agent = Sarsa(env) # SARSA Agent\n",
        "  list_cumulative_rewards = [] # list of cumulative rewards of all episodes\n",
        "  list_number_of_actions = [] # list of number of actions of all episodes\n",
        "\n",
        "  for ep in range (number_of_episodes):\n",
        "    sum_reward = 0\n",
        "    done = False\n",
        "    number_of_action = 0  # Current number of actions have been taken in this episode\n",
        "    while not done:\n",
        "      # Get current state\n",
        "      state = env.observe()\n",
        "\n",
        "      # Get next action\n",
        "      action = agent.get_action(state)\n",
        "\n",
        "      # Get the next state that the agent enters after taking that action\n",
        "      state2, reward, done, info = env.step(action)\n",
        "      \n",
        "      # Get next action from state2\n",
        "      action2 = agent.get_action(state2)\n",
        "      \n",
        "      # Update Q-table \n",
        "      agent.update(state, state2, reward, action, action2)\n",
        "      \n",
        "      # Visualize how the agent plays in the last episode\n",
        "      if ep==number_of_episodes-1:\n",
        "        i, j = np.unravel_index(action2, (env.grid_size, env.grid_size))\n",
        "        print(\"Action: (\" + str(j) + \") Reward: \", reward)\n",
        "      \n",
        "      # Update reward and number of actions\n",
        "      sum_reward += reward\n",
        "      number_of_action += 1\n",
        "    list_cumulative_rewards.append(sum_reward)\n",
        "    list_number_of_actions.append(number_of_action)\n",
        "    \n",
        "    # Print log\n",
        "    print(\"Episode \", ep+1, \" Cumulative reward: \", sum_reward, \" Number of actions: \", number_of_action)\n",
        "    \n",
        "    # Reset env after each episode\n",
        "    env.reset()\n",
        "\n",
        "    # Epsilon is decayed since the agent is getting more and more knowledge\n",
        "    agent.epsilon = agent.epsilon * agent.epsilon_decay_rate\n",
        "  \n",
        "  # Plot or download the results\n",
        "  plot_results(list_cumulative_rewards, list_number_of_actions, \"SARSA\", download_plots)\n",
        "\n"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAHmKR3heU5h"
      },
      "source": [
        "# Deep Q Network (pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItXCligteZwj"
      },
      "source": [
        "class Replay():\n",
        "  \"\"\"\n",
        "    Memory for storing experience \n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.buffer = []\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    \"\"\"\n",
        "      Add a new experience to the buffer\n",
        "      :param state: The state before taking the action\n",
        "      :param action: action taken\n",
        "      :param reward: Reward for taking that action\n",
        "      :param next_state: The state that the agent enters after taking the action\n",
        "      :param done: Whether the agent finishes the game\n",
        "    \"\"\"\n",
        "    self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    \"\"\"\n",
        "      Return a batch of samples from the experience buffer\n",
        "      :param batch_size: The number of sample that you want to take\n",
        "      :return the batch of samples but decomposed into lists of states, actions, rewards, next_states \n",
        "    \"\"\"\n",
        "    states, actions, rewards, next_states = [], [], [], []\n",
        "\n",
        "    # Random samples\n",
        "    samples = random.sample(self.buffer, batch_size)\n",
        "\n",
        "    for s in samples:\n",
        "      s = np.asarray(s)\n",
        "      state = torch.from_numpy(s[0])\n",
        "      action = torch.tensor(s[1])\n",
        "      reward = torch.tensor(s[2])\n",
        "      next_state = torch.from_numpy(s[3])\n",
        "      states.append(state)\n",
        "      actions.append(action)\n",
        "      rewards.append(reward)\n",
        "      next_states.append(next_state) \n",
        "    return states, actions, rewards, next_states\n",
        "\n",
        "class DQN():\n",
        "  \"\"\"\n",
        "    Model a DQN agent\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    self.epsilon = 1.0 # For deciding exploitation or exploration\n",
        "    self.epsilon_decay_rate = 0.99 # Epsilon is decayed after each episode with a fixed rate\n",
        "    self.discount = 0.5 # The weight for future rewards\n",
        "    self.env = env # The environment where the agent acts\n",
        "    self.device = torch.device('cuda')\n",
        "    \n",
        "    # Main network \n",
        "    self.main_dqn = torch.nn.Sequential(torch.nn.Linear(3, 64),\n",
        "                                        torch.nn.LeakyReLU(),\n",
        "                                        torch.nn.Linear(64, 128),\n",
        "                                        torch.nn.LeakyReLU(),\n",
        "                                        torch.nn.Linear(128,self.env.nA))\n",
        "    # Target network\n",
        "    self.target_dqn = torch.nn.Sequential(torch.nn.Linear(3, 64),\n",
        "                                          torch.nn.LeakyReLU(),\n",
        "                                          torch.nn.Linear(64, 128),\n",
        "                                          torch.nn.LeakyReLU(),\n",
        "                                          torch.nn.Linear(128,self.env.nA))\n",
        "    # Send models to GPU\n",
        "    self.main_dqn.to(self.device)\n",
        "    self.target_dqn.to(self.device)\n",
        "\n",
        "    # Optimizer and Loss function\n",
        "    self.optimizer = torch.optim.Adam(self.main_dqn.parameters(), lr=1e-4)\n",
        "    self.mse = torch.nn.MSELoss()\n",
        "\n",
        "  def select_action(self, state):\n",
        "    \"\"\"\n",
        "      Return an action to take based on epsilon (greedy or random action)\n",
        "      :param state: the current state\n",
        "      :return action: next action to take\n",
        "    \"\"\"\n",
        "    state = np.asarray(state)\n",
        "    random_number = np.random.uniform()\n",
        "    if random_number < self.epsilon:\n",
        "      # Random action\n",
        "      return np.random.randint(0, self.env.grid_size*self.env.grid_size)\n",
        "    else:\n",
        "      # Greedy action\n",
        "      state = torch.from_numpy(state).to(self.device, dtype=torch.float)\n",
        "      return torch.argmax(self.main_dqn(state)).item()\n",
        "       \n",
        "  def train(self, states, actions, rewards, next_states):\n",
        "    \"\"\"\n",
        "      Train the network with a batch of samples\n",
        "      :param states: The state before taking the action\n",
        "      :param actions: action taken\n",
        "      :param rewards: Reward for taking that action\n",
        "      :param next_states: The state that the agent enters after taking the action\n",
        "      :return loss: the loss value after training the batch of samples\n",
        "    \"\"\"\n",
        "    # Send data to GPU\n",
        "    states = torch.stack(states).to(self.device, dtype=torch.float)\n",
        "    actions = torch.stack(actions).to(self.device)\n",
        "    rewards = torch.stack(rewards).to(self.device, dtype=torch.float)\n",
        "    next_states = torch.stack(next_states).to(self.device, dtype=torch.float)\n",
        "\n",
        "    # Calculate target Q values using the Target Network  \n",
        "    next_qs = self.target_dqn(next_states)\n",
        "    max_next_qs = torch.max(next_qs, axis=-1)  \n",
        "    target = rewards + max_next_qs[0]*self.discount\n",
        "\n",
        "    # Calculate Q values using the Main Network\n",
        "    action_masks = torch.nn.functional.one_hot(actions, self.env.nA)\n",
        "    q_value = torch.sum(action_masks*self.main_dqn(states), dim=1)\n",
        "    target = target.detach()\n",
        "    \n",
        "    # Calculate MSE loss\n",
        "    loss = self.mse(target, q_value)\n",
        "\n",
        "    # Optimize the model\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def runner(self, number_of_episodes, download_plots):\n",
        "    \n",
        "    batch_size = 256              # Size of the batch of samples\n",
        "    buffer = Replay()             # Experience Buffer\n",
        "    list_cumulative_rewards = []  # List of cumulative rewards in each episode\n",
        "    list_number_of_actions = []   # List of number of actions in each episode\n",
        "\n",
        "    for episode in range(number_of_episodes+1):\n",
        "      ep_reward = 0         # Reward for this episode\n",
        "      done = False          # Whether the game is finished\n",
        "      number_of_action = 0  # Number of action in this episode\n",
        "\n",
        "      # Reset the env\n",
        "      self.env.reset()\n",
        "\n",
        "      # Get the current state\n",
        "      state = self.env.observe()\n",
        "      state = np.asarray(state)\n",
        "\n",
        "      while not done:\n",
        "        # Get and execute the next action for the current state\n",
        "        action = self.select_action(state)\n",
        "        next_state, reward, done, info = env.step(np.unravel_index(action,(self.env.grid_size,self.env.grid_size)))\n",
        "        ep_reward += reward\n",
        "        next_state = np.asarray(next_state)\n",
        "\n",
        "        # Save what the agent just learnt to the experience buffer.\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        state = next_state\n",
        "\n",
        "        # Every 200 actions: Copy the Main Network's weights to the Target Network \n",
        "        if number_of_action>0 and (number_of_action%200==0) :\n",
        "          self.target_dqn.load_state_dict(self.main_dqn.state_dict())\n",
        "\n",
        "        # Start training once the size of the buffer greater than the batch size\n",
        "        if len(buffer) >= batch_size:\n",
        "          states, actions, rewards, next_states = buffer.sample(batch_size)\n",
        "          loss = self.train(states, actions, rewards, next_states)\n",
        "        \n",
        "        number_of_action += 1\n",
        "\n",
        "      # Print log\n",
        "      print(f'Episode {episode}/{number_of_episodes}. Epsilon: {self.epsilon:.3f}. Number of actions: {number_of_action}. Loss: {loss} '\n",
        "            f'Reward: {ep_reward}')\n",
        "      \n",
        "      list_cumulative_rewards.append(ep_reward)\n",
        "      list_number_of_actions.append(number_of_action)\n",
        "      \n",
        "      # Epsilon is decayed since the agent is getting more and more knowledge\n",
        "      self.epsilon = self.epsilon * self.epsilon_decay_rate\n",
        "\n",
        "    # Close the env and plot or download the results\n",
        "    env.close()\n",
        "    plot_results(list_cumulative_rewards, list_number_of_actions, \"Deep-QN\", download_plots)\n",
        "\n"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GsIwCy5IjTK"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT14kyihWLc0"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  number_of_episodes = 3000  # Number of episodes you want to run the agent\n",
        "  download_plots = False     # \"True\" if this notebook is running on Google Colab and you want to save the figures to your computer, \"False\" otherwise\n",
        "  env = env_generator()\n",
        "\n",
        "  # Test Q-Learning agent\n",
        "  env.reset()\n",
        "  test_Q_Learning(env, number_of_episodes, download_plots)\n",
        "\n",
        "  # Test SARSA agent\n",
        "  env.reset()\n",
        "  test_SARSA(env, number_of_episodes, download_plots)\n",
        "\n",
        "  # Test DQN agent \n",
        "  env.reset()\n",
        "  dqn = DQN(env)\n",
        "  dqn.runner(number_of_episodes, download_plots)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}