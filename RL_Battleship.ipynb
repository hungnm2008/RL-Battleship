{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RL-Battleship",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL_7O-oQUcaQ"
      },
      "source": [
        "!pip install gym\n",
        "!pip install tensorflow\n",
        "!pip install stable-baselines\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "from stable_baselines.common.env_checker import check_env\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3F2i3OPmGoq"
      },
      "source": [
        "# Battleship Environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oSYkWwfVB7S"
      },
      "source": [
        "class BatteshipEnv(gym.Env):\n",
        "  def __init__(self, grid_size, ships_board):\n",
        "    super(BatteshipEnv, self).__init__()\n",
        "    self.grid_size = grid_size  # With grid_size = n we have a board of n*n\n",
        "    self.max_health = int(np.max(ships_board)) # The maximum health h among the ships\n",
        "    self.observation_space = spaces.Discrete(self.grid_size * self.grid_size) # Obeservation space\n",
        "    self.action_space = spaces.Tuple((spaces.Discrete(self.grid_size),spaces.Discrete(self.grid_size))) # Action space\n",
        "    self.nS = self.grid_size * self.grid_size * (self.max_health+3) # Size of state space n*n*h\n",
        "    self.nA = self.grid_size * self.grid_size # Size of action space n*n\n",
        "    self.ships_board = ships_board # Given board with placed ships\n",
        "    self.board = np.zeros((self.grid_size,self.grid_size), dtype='int') # Currently playing board\n",
        "    self.s = (0,0,0)  # Currently state: a tuple representing (x,y,health)\n",
        "    self.number_of_hit_to_win = np.sum(self.ships_board) # Number of hit to win the game = sum of health of all ships   \n",
        "    self.number_of_hit = 0  # Current number of hit \n",
        "    self.destroyed = int(self.max_health + 1)\n",
        "    self.empty =  int(self.max_health + 1)\n",
        "    \n",
        "  def calculate_reward(self, action):\n",
        "    \"\"\" return reward for an action\n",
        "    \"\"\" \n",
        "    x = action[0]\n",
        "    y = action[1]\n",
        "    reward = 0\n",
        "    if (self.board[x,y] == self.empty) | (self.board[x,y] == self.destroyed) :\n",
        "      reward -= 1\n",
        "    return reward\n",
        "\n",
        "  def update_board(self, action):\n",
        "    x = action[0]\n",
        "    y = action[1]\n",
        "\n",
        "    if self.board[x,y] == self.empty:\n",
        "      self.s = (action[0], action[1], int(self.empty))\n",
        "    elif self.board[x,y] == self.destroyed:\n",
        "      self.s = (action[0], action[1], int(self.destroyed))\n",
        "    else:\n",
        "      if self.ships_board[x,y] != 0:\n",
        "        if self.board[x,y] < self.ships_board[x,y]:\n",
        "          self.board[x,y] += 1 # One more hit\n",
        "          self.number_of_hit += 1\n",
        "        else:\n",
        "          self.board[x,y] = self.destroyed\n",
        "      else:\n",
        "        self.board[x,y] = self.empty\n",
        "      self.s = (action[0], action[1], int(self.board[x,y]))\n",
        "    return self.s\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\" input: action\n",
        "        return: next_state, reward, done, info\n",
        "    \"\"\"\n",
        "    next_state = self.update_board(action)\n",
        "    reward = self.calculate_reward(action)\n",
        "    done = bool(self.number_of_hit == self.number_of_hit_to_win)\n",
        "    info = {}\n",
        "    return next_state, reward, done, info\n",
        "\n",
        "  def render(self, mode='human'):\n",
        "    for i in range(self.grid_size):\n",
        "      print(\"-------------------------------------------\")\n",
        "      line = \"\"\n",
        "      \n",
        "      for j in range(self.grid_size):     \n",
        "        line += \" | \"\n",
        "        if self.board[i,j] == -1:\n",
        "          line += \"O\"\n",
        "        elif self.board[i,j] == 0:\n",
        "          line += \" \"\n",
        "        else:\n",
        "          line += str(int(self.board[i,j]))\n",
        "\n",
        "      line += \" | \"\n",
        "      print(line)\n",
        "  def observe(self):\n",
        "    return self.s\n",
        "\n",
        "  def reset(self):\n",
        "    self.board = np.zeros((self.grid_size,self.grid_size))\n",
        "    self.number_of_hit = 0\n",
        "    return self.board\n",
        "\n",
        "def plot_results(list_cumulative_rewards, list_cumulative_steps, algo_name, download_plots):\n",
        "  # for reward in zip(list_cumulative_rewards):\n",
        "  plt.plot(list_cumulative_rewards, label=\"legend\")\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Cumulative rewards')\n",
        "  if download_plots:\n",
        "    plt.savefig(algo_name, format='eps', dpi=1200)\n",
        "    files.download(algo_name)\n",
        "    plt.clf()\n",
        "  else:\n",
        "    plt.show()\n",
        "  plt.plot(list_cumulative_steps, label=\"legend\")\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Number of actions to finish the game')\n",
        "  if download_plots:\n",
        "    plt.savefig(algo_name, format='eps', dpi=1200)\n",
        "    files.download(algo_name)\n",
        "    plt.clf() \n",
        "  else:\n",
        "    plt.show()\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duKgmzJImVJ_"
      },
      "source": [
        "# Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAcxGcQrnkd8"
      },
      "source": [
        "class QLearning():\n",
        "  def __init__(self, env):\n",
        "    self.env = env\n",
        "    self.learning_rate = 0.05\n",
        "    self.epsilon = 0.9\n",
        "    self.epsilon_decay_rate = 0.99\n",
        "    self.discount = 0.5\n",
        "\n",
        "    self.Q_values = self.Q_values = np.zeros([self.env.nS, self.env.nA])\n",
        "\n",
        "  def update(self, state, action, reward):\n",
        "    \"\"\" Update Q-table\n",
        "        :param state: previous state before taking the action (x,y,h)\n",
        "        :param action: action taken (x,y)\n",
        "        :param reward: reward for taking action a from state s\n",
        "    \"\"\"\n",
        "    new_state = self.env.observe()\n",
        "    idx_new_state = np.ravel_multi_index([new_state[0], new_state[1], new_state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "\n",
        "    idx_state = np.ravel_multi_index([state[0], state[1], state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    idx_action = np.ravel_multi_index([action[0], action[1]],(env.grid_size, env.grid_size))\n",
        "\n",
        "    #Update Q values\n",
        "    self.Q_values[idx_state][idx_action] = self.Q_values[idx_state][idx_action] + self.learning_rate * (reward + self.discount * max(self.Q_values[idx_new_state]) - self.Q_values[idx_state][idx_action])\n",
        "\n",
        "  def get_action(self, state):\n",
        "    \"\"\" Return an action to take based on epsilon (greedy or random action)\n",
        "        :param state: current state\n",
        "        :return action: action to take in the next time step\n",
        "    \"\"\"\n",
        "    # Choose random action or best action\n",
        "    idx_state = np.ravel_multi_index([state[0], state[1], state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    random_number = np.random.uniform()\n",
        "    if random_number < self.epsilon:\n",
        "      return (np.random.randint(0, self.env.grid_size), np.random.randint(0, self.env.grid_size)) # Random action\n",
        "    else:\n",
        "      idx_action = np.argmax(self.Q_values[idx_state]) # Greedy action\n",
        "      return np.unravel_index(idx_action,(self.env.grid_size,self.env.grid_size))\n",
        "\n",
        "def test_Q_Learning(number_of_episodes, download_plots):\n",
        "  env = BatteshipEnv(grid_size, ships_board)\n",
        "  agent = QLearning(env)  \n",
        "  list_cumulative_rewards = []\n",
        "  list_cumulative_steps = []  \n",
        "  for ep in range (number_of_episodes):\n",
        "    sum_reward = 0\n",
        "    done = False\n",
        "    number_of_steps = 0  \n",
        "    while not done:\n",
        "      current_state = env.observe()\n",
        "      action = agent.get_action(current_state)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      agent.update(current_state, action, reward)\n",
        "      if ep==number_of_episodes-1:\n",
        "        i, j = np.unravel_index(action, (env.grid_size, env.grid_size))\n",
        "        print(\"Action: (\" + str(j) + \") Reward: \", reward)\n",
        "      sum_reward += reward\n",
        "      number_of_steps += 1\n",
        "    list_cumulative_rewards.append(sum_reward)\n",
        "    list_cumulative_steps.append(number_of_steps)\n",
        "    print(\"Episode \", ep+1, \" Cumulative reward: \", sum_reward, \" Number of actions: \", number_of_steps)\n",
        "    env.reset()\n",
        "    # epsilon is decayed since the agent is having more and more knowledge\n",
        "    agent.epsilon = agent.epsilon * agent.epsilon_decay_rate\n",
        "  plot_results(list_cumulative_rewards, list_cumulative_steps, \"Q-Learning\", download_plots)\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WssJXj55dex2"
      },
      "source": [
        "# SARSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmca671mgLbx"
      },
      "source": [
        "class Sarsa():\n",
        "  def __init__(self, env):\n",
        "    self.env = env\n",
        "    self.learning_rate = 0.05\n",
        "    self.epsilon = 0.9\n",
        "    self.epsilon_decay_rate = 0.99\n",
        "    self.discount = 0.5\n",
        "\n",
        "    self.Q_values = np.zeros([self.env.nS, self.env.nA])\n",
        "\n",
        "  def update(self, state, state2, reward, action, action2):\n",
        "    \"\"\" Update Q-table\n",
        "        :param state: previous state before taking the action\n",
        "        :param action: action taken\n",
        "        :param reward: reward for taking action a from state s\n",
        "    \"\"\"\n",
        "    \n",
        "    idx_state = np.ravel_multi_index([state[0], state[1], state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    idx_state2 = np.ravel_multi_index([state2[0], state2[1], state2[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    idx_action = np.ravel_multi_index([action[0], action[1]],(env.grid_size, env.grid_size))\n",
        "    idx_action2 = np.ravel_multi_index([action2[0], action2[1]],(env.grid_size, env.grid_size))\n",
        "\n",
        "    #Update Q values\n",
        "    self.Q_values[idx_state][idx_action] = self.Q_values[idx_state][idx_action] \\\n",
        "                                          + self.learning_rate * (reward + self.discount * self.Q_values[idx_state2][idx_action2] - self.Q_values[idx_state][idx_action])\n",
        "\n",
        "  def get_action(self, state):\n",
        "    \"\"\" Return an action to take based on epsilon (greedy or random action)\n",
        "        :param state: current state\n",
        "        :return action: action to take in the next time step\n",
        "    \"\"\"\n",
        "    # Choose random action or best action\n",
        "    idx_state = np.ravel_multi_index([state[0], state[1], state[2]],(env.grid_size, env.grid_size, env.max_health+3))\n",
        "    random_number = np.random.uniform()\n",
        "    if random_number < self.epsilon:\n",
        "      return (np.random.randint(0, self.env.grid_size), np.random.randint(0, self.env.grid_size)) # Random action\n",
        "    else:\n",
        "      idx_action = np.argmax(self.Q_values[idx_state]) # Greedy action\n",
        "      return np.unravel_index(idx_action,(self.env.grid_size,self.env.grid_size))\n",
        "\n",
        "def test_SARSA(number_of_episodes, download_plots):\n",
        "  env = BatteshipEnv(grid_size, ships_board)\n",
        "  agent = Sarsa(env)\n",
        "  list_cumulative_rewards = []\n",
        "  list_cumulative_steps = []\n",
        "\n",
        "  for ep in range (number_of_episodes):\n",
        "    sum_reward = 0\n",
        "    done = False\n",
        "    number_of_steps = 0  \n",
        "    while not done:\n",
        "      state = env.observe()\n",
        "      action = agent.get_action(state)\n",
        "      state2, reward, done, info = env.step(action)\n",
        "      action2 = agent.get_action(state2)\n",
        "      agent.update(state, state2, reward, action, action2)\n",
        "      if ep==number_of_episodes-1:\n",
        "        i, j = np.unravel_index(action2, (env.grid_size, env.grid_size))\n",
        "        print(\"Action: (\" + str(j) + \") Reward: \", reward)\n",
        "      sum_reward += reward\n",
        "      number_of_steps += 1\n",
        "    list_cumulative_rewards.append(sum_reward)\n",
        "    list_cumulative_steps.append(number_of_steps)\n",
        "    print(\"Episode \", ep+1, \" Cumulative reward: \", sum_reward, \" Number of actions: \", number_of_steps)\n",
        "    env.reset()\n",
        "    # epsilon is decayed since the agent is having more and more knowledge\n",
        "    agent.epsilon = agent.epsilon * agent.epsilon_decay_rate\n",
        "  \n",
        "  plot_results(list_cumulative_rewards, list_cumulative_steps, \"SARSA\", download_plots)\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GsIwCy5IjTK"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT14kyihWLc0",
        "outputId": "74ad2827-c733-4393-9807-1888fb5b4ea4"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  grid_size = 10\n",
        "  ships_board = np.zeros((grid_size, grid_size), dtype='int')\n",
        "  ships_board[0,9] = 1 \n",
        "  ships_board[1,3] = 2 \n",
        "  ships_board[7,2] = 2 \n",
        "  ships_board[3,1] = 2\n",
        "  ships_board[6,7] = 2\n",
        "  ships_board[3,5] = 3 \n",
        "  ships_board[9,9] = 3 \n",
        "  ships_board[4,5] = 3 \n",
        "  ships_board[9,4] = 3\n",
        "  ships_board[5,8] = 3\n",
        "  print(ships_board)\n",
        "\n",
        "  ### Test Q-Learning + SARSA ###\n",
        "  number_of_episodes = 2000\n",
        "  download_plots = False\n",
        "  test_Q_Learning(number_of_episodes, download_plots)\n",
        "  test_SARSA(number_of_episodes, download_plots)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 2 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 2 0 0 0 3 0 0 0 0]\n",
            " [0 0 0 0 0 3 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 3 0]\n",
            " [0 0 0 0 0 0 0 2 0 0]\n",
            " [0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 3 0 0 0 0 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}